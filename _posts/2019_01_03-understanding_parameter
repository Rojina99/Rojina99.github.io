---
title: "Learning Parameters"
date: 2019-01-03
tags: [machine learning, data science, neural network, activation function]
header:
  image: "/images/introduction/intro.jpg"
excerpt: "Machine Learning, Data Science"
mathjax: "true"
gallery:
   - image_path: /images/parameters/gd.png
     alt: "activation ann"
     url: /images/parameters/gd.png
     title: "Figure 1"
     - image_path: /images/parameters/lr_gd.png
     alt: "activation ann"
     url: /images/parameters/lr_gd.png
     title: "Figure 1"
---

We have to understand that preparing a simple network is very easy task once you get the grasp of network structure. Now a days there are so many high level programming APIs that have already implemented this feature. 
One of the major platfom to work on machine learning is Tensorflow which also support Keras as higher level abstraction. But it is better to prepare a simple network by ourself at first. So, that this understanding will help us while working with higher level APIs.

Then what makes the process of getting required output from artificial neural network so difficult then?
It is because of parameters that a network is comprised of. There are many parameters we have to consider while preparing a network. Some of the basic parmaeters are:
learning rate, number of layers of neural network, number of neurons in a layer, optimizer function, which kind of network is more suitable for a specific purpose and features that must be passed as input to the network.
We also must be familiar about data preparation process before passing them to neural network.Data preparation is a vague process so we will discuss about it in a separate post.

Now let's discuss about each parameter in detail.

1. Learning Rate: This is the rate at which our network learns with provided data. We have be careful while chossing a leraning rate. If learning rate is too high the network will not converge. 
And if learning rate is too small network might take infinite time to converge i.e. find local mimina in case of gradient descent optimization.

From figure we can see a simple illustration of gradient descent algorithm optimization method. We can know from the figure that purpose of optimizer function is to reduce loss as much as possible. So, it tries to find a minima called local minima.  
It is better to find global minimum. But this comes as drawback of gradient descent optimizer since it is gets stuck in local miminum.

We also can see that if learning rate is too small it takes forever for a network to reach local minum and if lea
